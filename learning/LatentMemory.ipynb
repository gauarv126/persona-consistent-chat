{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e40772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "k = 16         # number of memory slots\n",
    "d = 768        # hidden size (same as BART-base)\n",
    "\n",
    "# 1. Learnable memory matrix\n",
    "M = nn.Parameter(torch.randn(k, d))   # <<< THIS is your memory!\n",
    "\n",
    "# 2. h_z from encoder (just an example here)\n",
    "h_z = torch.randn(d)  # shape: [768]\n",
    "\n",
    "# 3. Linear projection: h_z -> logits over memory slots\n",
    "W = nn.Linear(d, k)   # maps 768 -> 16\n",
    "logits = W(h_z)       # shape: [16]\n",
    "\n",
    "# 4. Attention weights over memory\n",
    "alpha = torch.softmax(logits, dim=0)  # shape: [16]\n",
    "\n",
    "# 5. Weighted sum over memory slots\n",
    "#       alpha: [16], M: [16, 768]\n",
    "#       → weighted sum across memory rows\n",
    "z = torch.sum(alpha.unsqueeze(1) * M, dim=0)  # shape: [768]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add29e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1]), torch.Size([16, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.unsqueeze(1).shape, M.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18754fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8ffa0",
   "metadata": {},
   "source": [
    "### Here is Link to learn more\n",
    "[Learning PDF](https://chatgpt.com/share/67f3649b-fef0-8013-9c1c-2e3727affd56) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15eb8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed099fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_z:\n",
      " tensor([[0.2000, 0.4000, 0.6000, 0.8000]])\n",
      "logits:\n",
      " tensor([0.4800, 0.4200, 0.0200])\n",
      "attention weights (alpha):\n",
      " tensor([0.3886, 0.3660, 0.2453])\n",
      "final memory vector (z):\n",
      " tensor([0.3411, 0.2589, 0.1102, 0.1902])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dimensions\n",
    "k = 3    # number of memory slots\n",
    "d = 4    # hidden size\n",
    "\n",
    "# Example hidden vector h_z (1 x d)\n",
    "h_z = torch.tensor([[0.2, 0.4, 0.6, 0.8]], dtype=torch.float32)  # shape: (1, 4)\n",
    "\n",
    "# Memory matrix M (k x d)\n",
    "M = torch.tensor([\n",
    "    [0.5, 0.1, 0.0, 0.3],\n",
    "    [0.2, 0.4, 0.1, 0.0],\n",
    "    [0.3, 0.3, 0.3, 0.3]\n",
    "], dtype=torch.float32)  # shape: (3, 4)\n",
    "\n",
    "# Linear layer weights W (k x d) and bias (k)\n",
    "W = torch.tensor([\n",
    "    [ 0.1, 0.0,  0.2, 0.3],\n",
    "    [-0.2, 0.5, -0.1, 0.4],\n",
    "    [ 0.3, 0.2,  0.0, 0.1]\n",
    "], dtype=torch.float32)  # shape: (3, 4)\n",
    "\n",
    "b = torch.tensor([0.1, 0.0, -0.2], dtype=torch.float32)  # shape: (3,)\n",
    "\n",
    "# Step 1: Compute logits\n",
    "logits = torch.matmul(W, h_z.T).squeeze(1) + b  # shape: (3,)\n",
    "\n",
    "# Step 2: Softmax to get attention weights\n",
    "alpha = F.softmax(logits, dim=0)  # shape: (3,)\n",
    "\n",
    "# Step 3: Compute z as weighted sum over memory M\n",
    "z = torch.sum(alpha.unsqueeze(1) * M, dim=0)  # shape: (4,)\n",
    "\n",
    "# Print results\n",
    "print(\"h_z:\\n\", h_z)\n",
    "print(\"logits:\\n\", logits)\n",
    "print(\"attention weights (alpha):\\n\", alpha)\n",
    "print(\"final memory vector (z):\\n\", z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8bdba3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4800, 0.4200, 0.0200]), tensor([0.3886, 0.3660, 0.2453]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits,alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d535037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce7a5706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MemoryAttentionModel(nn.Module):\n",
    "    def __init__(self, k=16, d=768):\n",
    "        super(MemoryAttentionModel, self).__init__()\n",
    "        \n",
    "        # Learnable memory matrix\n",
    "        self.M = nn.Parameter(torch.randn(k, d))  # Memory slots of size [16, 768]\n",
    "\n",
    "        # Linear projection from hidden state to logits over memory slots\n",
    "        self.W = nn.Linear(d, k)  # Linear layer to map from 768 to 16 (logits)\n",
    "\n",
    "    def forward(self, h_z):\n",
    "        # h_z is the encoder output (e.g., from BART or any encoder) of size [768]\n",
    "        \n",
    "        # Step 1: Compute logits over memory slots\n",
    "        logits = self.W(h_z)  # shape: [16]\n",
    "\n",
    "        # Step 2: Compute attention weights over memory slots (softmax)\n",
    "        alpha = torch.softmax(logits, dim=0)  # shape: [16]\n",
    "\n",
    "        # Step 3: Weighted sum of memory slots using attention weights\n",
    "        z = torch.sum(alpha.unsqueeze(1) * self.M, dim=0)  # shape: [768]\n",
    "\n",
    "        return z\n",
    "\n",
    "# Example usage:\n",
    "model = MemoryAttentionModel(k=16, d=768)\n",
    "h_z = torch.randn(768)  # Example encoder output (hidden state) of shape [768]\n",
    "output = model(h_z)  # Output will be a weighted sum of memory slots of shape [768]\n",
    "\n",
    "print(output.shape)  # Should print torch.Size([768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cafbb70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MemoryAttentionModel                     [1024]                    10,240\n",
       "├─Linear: 1-1                            [10]                      10,250\n",
       "==========================================================================================\n",
       "Total params: 20,490\n",
       "Trainable params: 20,490\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "class MemoryAttentionModel(nn.Module):\n",
    "    def __init__(self, k=16, d=768):\n",
    "        super(MemoryAttentionModel, self).__init__()\n",
    "        \n",
    "        # Learnable memory matrix\n",
    "        self.M = nn.Parameter(torch.randn(k, d))  # Memory slots of size [16, 768]\n",
    "\n",
    "        # Linear projection from hidden state to logits over memory slots\n",
    "        self.W = nn.Linear(d, k)  # Linear layer to map from 768 to 16 (logits)\n",
    "\n",
    "    def forward(self, h_z):\n",
    "        # h_z is the encoder output (e.g., from BART or any encoder) of size [768]\n",
    "        \n",
    "        # Step 1: Compute logits over memory slots\n",
    "        logits = self.W(h_z)  # shape: [16]\n",
    "\n",
    "        # Step 2: Compute attention weights over memory slots (softmax)\n",
    "        alpha = torch.softmax(logits, dim=0)  # shape: [16]\n",
    "\n",
    "        # Step 3: Weighted sum of memory slots using attention weights\n",
    "        z = torch.sum(alpha.unsqueeze(1) * self.M, dim=0)  # shape: [768]\n",
    "\n",
    "        return z\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MemoryAttentionModel(k=10, d=1024)\n",
    "\n",
    "# Use torchinfo to display the model summary with input shape [768] (without batch size dimension)\n",
    "summary(model, input_size=(1024,))  # Corrected input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90bdccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmedr_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
